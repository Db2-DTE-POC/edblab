{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using EDB Postgres with Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a tutorial on how to connect to EDB Postgres from a Jupyter Python notebook. It includes how to run SQL queries from a Jupyter notebook, transfer results to a Dataframe and use Pandas to analyze and graph data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Python and Magic\n",
    "#### Should already be completed as part of the EDB Postgres Hands on Lab\n",
    "If you completed the rest of the EDB Postgres lab you should have already setup EDB Postgres to work with the Python language. If you are setting up your own system, you need to install the following libraries using pip and apt-get\n",
    "1. ipython-sql: Postgres Magic commands for Jupyter notebooks\n",
    "2. libpq-dev: The PostgreSQL C development library\n",
    "3. psycopg: A DB API 2.0 compliant PostgreSQL driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Jupyter SQL Module\n",
    "The next cell loads the Jupyter SQL Magic module (ipython-sql) into your system into this Juyter notebook. \n",
    "* Click on the next cell and click **Run** from the menu bar above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything loaded correctly you should see a **1** inside the square brackets beside the cell above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell uses an **%sql** magic command to connect to your **stocks** EDB Postgres database. \n",
    "\n",
    "The command below includes all information to connect to any EDB Posgres or PostgreSQL database. Here are the values we are using in this Hands-On Lab:\n",
    "\n",
    "**Database type:** postgresql\n",
    "\n",
    "**User ID:** enterprisedb\n",
    "\n",
    "**Password:** 123qwe123\n",
    "\n",
    "**URL:** localhost\n",
    "\n",
    "**Port:** 5444\n",
    "\n",
    "**Database:** stocks\n",
    "\n",
    "\n",
    "* Click on the next cell and click **Run** from the menu bar above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://enterprisedb:123qwe123@localhost:5444/stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **%sql** magic command it is easy to submit SQL statements and see the results of a query. The next cell retrieves the last name and birthday of ten customers. \n",
    "* Run the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select customerid, lastname, birthdate from customer fetch first 10 rows only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make longer SQL statements and queries easier to read, use the **%%sql** magic command. You can format SQL across multiple lines. The next cell retrieves the ten youngest customers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select customerid, lastname, birthdate \n",
    "    from customer \n",
    "    order by birthdate desc\n",
    "    fetch first 10 rows only;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take advantage of the power of Python and Jupyter together, the next cell imports a number of useful Python libraries. \n",
    "* **pandas** provides a powerful in memory table object called a dataframe for manipulating tables of data\n",
    "* **psycopg2** supports the full Python DB API 2.0 specification and the thread safety\n",
    "* **sqlalchemy** is a Python SQL toolkit and Object Relational Mapper\n",
    "* **matplotlib** is a toolkit for graphing a plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use SQLAlchemy you need to construct a connection string and use that to create a connection to your EDB Postgres stocks database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "# EDB username, password, and database name\n",
    "ADDRESS = 'localhost' \n",
    "PORT = '5444'\n",
    "USERNAME = 'enterprisedb' \n",
    "PASSWORD = '123qwe123' \n",
    "DBNAME = 'stocks'\n",
    "# connection string\n",
    "edb_str = ('postgresql://{username}:{password}@{ipaddress}:{port}/{dbname}'\n",
    "          .format(username=USERNAME,\n",
    "           password=PASSWORD,\n",
    "           ipaddress=ADDRESS,\n",
    "           port=PORT,\n",
    "           dbname=DBNAME))\n",
    "# Create the connection\n",
    "cnx = create_engine(edb_str) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is similar to your previous SQL query. Except this time we don't include the **ORDER BY** clause and we omit the **FETCH FIRST 10 ROWS ONLY** clause. The results of the whole table including the customerid, lastname and birthdate are loaded into a Pandas Dataframe named **df**. The last line of the next cell displays the first 10 rows of that dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \\\n",
    "'''\n",
    "select customerid, lastname, birthdate \n",
    "    from customer \n",
    "'''\n",
    "df = pd.read_sql_query(sql, cnx) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new Dataframe **df** presists across cells. So you can sort the values in the dataframe for display to also show the 10 youngest customers. The next cell sorts by birthdate in descending order and returns the first 10 rows. Notice that we didn't have to go back to EDB Postgres for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.sort_values(by=['birthdate'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above didn't change the contents or order of the data in the dataframe. It simply returned a set of order results. Run the next cell to see that **df** has not changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the **df** dataframe you need to replace the current value with the sorted results. Try next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['birthdate'], ascending=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Stock Tables and more Complex Queries\n",
    "The key to using Jupyter and Python together with EDB Postgres is to use their strengths together. \n",
    "\n",
    "While you can simply load an entire table of data into a dataframe and manipulate data through Pandas, the powerful EDB Postgres SQL engine is far more efficient with large data sets. Make sure you do as much work where the data lives before you bring it to Jupyter. You don't want to maipulate GBs of data in Jupyter. Leave that to EDB Postgres. \n",
    "\n",
    "Conversely, once you have narrowed down the data you want to work with using SQL, Python and Pandas and libraries like matplotlib are very powerful, and let you explore data using powerful statistical analysis and graphing without having to go back to EDB Postgres and rerun expensive queries. \n",
    "\n",
    "Lets explore some of what we can do with the **stocks** data using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Tables\n",
    "Run the next cells to see the data available in each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM STOCK_SYMBOLS FETCH FIRST 3 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM STOCK_SYMBOLS FETCH FIRST 3 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM CUSTOMER FETCH FIRST 3 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT * FROM STOCK_HISTORY FETCH FIRST 3 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One week of trading history \n",
    "Now lets see what the stock transactions took place in the first week of December of 2018 for American Express. You can try replacing the **'AXP'** stock symbol with other symbols from the STOCK_SYMBOLS table and run the next cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT tx_date, custid, price*quantity::money as value FROM STOCK_TRANSACTIONS \n",
    "    WHERE SYMBOL = 'AXP'\n",
    "    AND TX_DATE::date >= '2018-12-01'\n",
    "    AND TX_DATE::date <= '2018-12-07' \n",
    "    ORDER BY value DESC\n",
    "FETCH FIRST 10 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bought/Sold Amounts of Top 10 stocks\n",
    "Here is an example of how to find the top ten stocks bought or sold in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "WITH BOUGHT(\n",
    "    SYMBOL,\n",
    "    AMOUNT\n",
    "  ) AS\n",
    "  (\n",
    "SELECT SYMBOL, SUM(QUANTITY)\n",
    "  FROM STOCK_TRANSACTIONS\n",
    "  WHERE QUANTITY > 0\n",
    "  GROUP BY SYMBOL\n",
    "  ), SOLD(\n",
    "    SYMBOL,\n",
    "    AMOUNT\n",
    "  ) AS\n",
    "  (\n",
    "  SELECT SYMBOL, -SUM(QUANTITY)\n",
    "  FROM STOCK_TRANSACTIONS\n",
    "  WHERE QUANTITY < 0\n",
    "  GROUP BY SYMBOL\n",
    "  )\n",
    "  SELECT B.SYMBOL, B.AMOUNT AS BOUGHT, S.AMOUNT AS SOLD\n",
    "  FROM BOUGHT B, SOLD S\n",
    "  WHERE B.SYMBOL = S.SYMBOL\n",
    "  ORDER BY B.AMOUNT DESC\n",
    "  FETCH FIRST 10 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stocks that Represent at least 3% of the Total Purchases during Week 45\n",
    "This query returns the top total purchases of stock in week 45 of multiple years, but only those that represent at least 3% of the total. It excludes the Dow Jones Industrial Average since it is an index an not a stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "WITH WEEK45(\n",
    "    SYMBOL,\n",
    "    PURCHASES\n",
    "  ) AS (\n",
    "  SELECT SYMBOL, SUM(VOLUME * CLOSE)::money\n",
    "  FROM STOCK_HISTORY\n",
    "  WHERE EXTRACT(WEEK FROM TX_DATE::date) =  45 AND SYMBOL <> 'DJIA'\n",
    "  GROUP BY SYMBOL\n",
    "), ALL45(TOTAL) AS (\n",
    "  SELECT SUM(PURCHASES) * .03\n",
    "  FROM WEEK45\n",
    ")\n",
    "  SELECT SYMBOL, PURCHASES\n",
    "  FROM WEEK45, ALL45\n",
    "  WHERE PURCHASES > TOTAL\n",
    "  ORDER BY PURCHASES DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top Trading Day\n",
    "We can figure out the top day of trading in the STOCK_HISTORY table based on the Dow Jones Industrial Average or DJIA index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "WITH MAX_VOLUME(AMOUNT) AS (\n",
    "  SELECT MAX(VOLUME) FROM STOCK_HISTORY\n",
    "    WHERE SYMBOL = 'DJIA'\n",
    ")\n",
    "  SELECT TX_DATE FROM STOCK_HISTORY, MAX_VOLUME M\n",
    "    WHERE SYMBOL = 'DJIA' AND VOLUME = M.AMOUNT\n",
    "FETCH FIRST 10 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compound Complex SQL\n",
    "EDB Postgres supports very complex and powerful SQL queries. The next example returns a list of the top ten customers in the US state of Ohio during the busiest trading day on record. The SQL takes advantage of subqueries to first determine the customers in Ohio, the top trading day and then calculates their total buy to find the to ten customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "WITH MAX_VOLUME(AMOUNT) AS (\n",
    "  SELECT MAX(VOLUME) FROM STOCK_HISTORY\n",
    "    WHERE SYMBOL = 'DJIA'\n",
    "),\n",
    "HIGHDATE(TX_DATE) AS (\n",
    "  SELECT TX_DATE FROM STOCK_HISTORY, MAX_VOLUME M\n",
    "    WHERE SYMBOL = 'DJIA' AND VOLUME = M.AMOUNT\n",
    "),\n",
    "CUSTOMERS_IN_OHIO(CUSTID) AS (\n",
    "  SELECT C.CUSTOMERID FROM CUSTOMER C \n",
    "    WHERE C.STATE = 'OH'\n",
    "),\n",
    "TOTAL_BUY(CUSTID,TOTAL) AS (\n",
    "  SELECT C.CUSTID, SUM(SH.QUANTITY * SH.PRICE) \n",
    "    FROM CUSTOMERS_IN_OHIO C, STOCK_TRANSACTIONS SH, HIGHDATE HD\n",
    "  WHERE SH.CUSTID = C.CUSTID AND\n",
    "        SH.TX_DATE = HD.TX_DATE AND \n",
    "        QUANTITY > 0 \n",
    "  GROUP BY C.CUSTID\n",
    ")\n",
    "  SELECT LASTNAME, CUSTOMERID, T.TOTAL::money\n",
    "    FROM CUSTOMER C, TOTAL_BUY T\n",
    "    WHERE C.CUSTOMERID = T.CUSTID\n",
    "    ORDER BY TOTAL DESC\n",
    "    FETCH FIRST 10 ROWS ONLY;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting together SQL, Pandas and Matplot\n",
    "Now that you understand the basics of running SQL, retrieving SQL into a dataframe and even running more complex SQL you can put it all together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by retrieving the high and low price of a single stock for 2018 and loading it into a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \\\n",
    "'''\n",
    "SELECT SYMBOL, TX_DATE, EXTRACT(DOY FROM TX_DATE::date) AS DAYOYEAR, HIGH, LOW, OPEN, CLOSE\n",
    "  FROM STOCK_HISTORY\n",
    "  WHERE SYMBOL IN ('AAPL' ) \n",
    "  AND EXTRACT(YEAR FROM TX_DATE::date) = 2018\n",
    "'''\n",
    "df = pd.read_sql_query(sql, cnx) \n",
    "df.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets graph the high and low prices on each day of the last 60 days of 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 60\n",
    "txdate= df['dayoyear'].tail(days)\n",
    "high = df['high'].tail(days)\n",
    "low = df['low'].tail(days)\n",
    "\n",
    "plt.xlabel(\"Day\", fontsize=12);\n",
    "plt.ylabel(\"High and Low Price\", fontsize=12);\n",
    "plt.suptitle(\"High and Low Price\", fontsize=20);\n",
    "plt.plot(txdate, high, 'r');\n",
    "plt.plot(txdate, low, 'b');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the powerful statistical methods in Pandas. The next cell adds an additional column with the rolling 30 day average price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SMA30\"] = df['close'].rolling(window=30).mean()\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the closing price of each day against the 30 day rolling average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = 60\n",
    "txdate= df['dayoyear'].tail(days)\n",
    "close = df['close'].tail(days)\n",
    "SMA30 = df['SMA30'].tail(days)\n",
    "\n",
    "plt.xlabel(\"Day\", fontsize=12);\n",
    "plt.ylabel(\"Close vs 30 Day MA\", fontsize=12);\n",
    "plt.suptitle(\"Close and 30 Day MA\", fontsize=20);\n",
    "plt.plot(txdate, close, 'r');\n",
    "plt.plot(txdate, SMA30, 'b');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out https://pandas.pydata.org/ to see everything you can do with the Pandas libraries. Using Jupyter, Python, Pandas and EDB Postgres is a powerful combination of capabilities that gives you a great start in analysing your data like a Data Scientist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you check out more Hands-On Labs at www.ibm.com/demos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
